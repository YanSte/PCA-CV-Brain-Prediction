{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yannicksteph/pca-cv-brain-prediction?scriptVersionId=144348195\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# | PCA | CV | Brain | Prediction |","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import offsetbox\nfrom mpl_toolkits import mplot3d\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import LocallyLinearEmbedding, MDS, Isomap, TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix, classification_report, make_scorer, accuracy_score, f1_score\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import make_scorer, precision_score, recall_score\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.feature_selection import RFECV, SelectKBest, f_classif\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.svm import SVC\nfrom xgboost.core import DMatrix\nimport xgboost as xgb","metadata":{"id":"e8MSiLR1CRQa","_kg_hide-output":false,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef get_k(filtered_features):\n  return int(len(filtered_features) * 0.75) #3 / 4 )\n\ndef plot_components(data, model, images=None, ax=None,\n                    thumb_frac=0.05, cmap='gray_r', prefit = False):\n    ax = ax or plt.gca()\n\n    if not prefit :\n        proj = model.fit_transform(data)\n    else:\n        proj = data\n    ax.plot(proj[:, 0], proj[:, 1], '.b')\n\n    if images is not None:\n        min_dist_2 = (thumb_frac * max(proj.max(0) - proj.min(0))) ** 2\n        shown_images = np.array([2 * proj.max(0)])\n        for i in range(data.shape[0]):\n            dist = np.sum((proj[i] - shown_images) ** 2, 1)\n            if np.min(dist) < min_dist_2:\n                # On ne montre pas le points trop proches\n                continue\n            shown_images = np.vstack([shown_images, proj[i]])\n            imagebox = offsetbox.AnnotationBbox(\n                offsetbox.OffsetImage(images[i], cmap=cmap),\n                                      proj[i])\n            ax.add_artist(imagebox)","metadata":{"id":"oAsDnK5iAPls","_kg_hide-output":false,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_folder_path = \"./dataset/\"\n#dataset_3d_path = \"/content/sample_data/3d_rsna_miccai_brain_tumor_brain_segmentation_pytorch_unet.csv\"\ndataset_3d_path = f\"{dataset_folder_path}3d_rsna_miccai_brain_tumor_brain_segmentation_pytorch_unet.csv\"\n\nexcluded_patient_ids = [109, 123, 709] #, 11, 351, 353, 442, 445, 554, 556, 568, 570, 572, 578, 581, 584, 587, 589, 593, 594, 596, 610]\n\ndataset_df = pd.read_csv(dataset_3d_path)\n\nexcluded_features_column_names = [\"LeastAxisLength\", \"Flatness\", \"gldm_GrayLevelNonUniformityNormalized\", \"gldm_DependencePercentage\"]\ndataset_df.drop(excluded_features_column_names, axis=1, inplace=True)\n\n# Patient BraTS21ID now is ID, and ID of Dataset\ndataset_df = dataset_df.set_index('ID')\n# Drop patient\ndataset_df = dataset_df.drop(index=excluded_patient_ids)","metadata":{"id":"fVaD_QxSChDH","_kg_hide-output":false,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntsne = TSNE(n_components=3)\nres = tsne.fit_transform(dataset_df.drop(\"MGMT_value\",axis=1))\nplt.figure(figsize=(10,5))\nax = plt.axes(projection='3d')\nax.scatter3D(res[:, 0], res[:, 1], res[:,2], c = dataset_df['MGMT_value'], cmap=plt.cm.Spectral, alpha = .7, s = 4) #**colorize)\nplt.axis('equal');\n","metadata":{"id":"DtwWQJe1zx4Z","outputId":"74d790c9-187c-4d20-a825-88d900f67cac","_kg_hide-output":false,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\n\nfeatures_selection = False\nif (features_selection) :\n  #matrice de corrélation\n  correlation_matrix = dataset_df.drop(\"MGMT_value\",axis=1).corr()\n  #print(correlation_matrix)\n\n  corr=0.75\n\n  highly_correlated_features = set()\n  # Identification des paires de caractéristiques fortement corrélées\n  for i in range(len(correlation_matrix.columns)):\n    for j in range(i):\n      if abs(correlation_matrix.iloc[i, j]) > corr:  # Définissez votre seuil de corrélation ici\n            colname_i = correlation_matrix.columns[i]\n            colname_j = correlation_matrix.columns[j]\n            if colname_i not in highly_correlated_features:\n                highly_correlated_features.add(colname_j)\n\n  # Suppression des caractéristiques fortement corrélées du jeu de données\n  dataset_df = dataset_df.drop(highly_correlated_features, axis=1)\n\n  # Sélection des features obtenues via la bibliothèque radiomics3d\n  selected_features = dataset_df.columns\n\n  # Séparation des features et du target\n  X = dataset_df[selected_features]\nelse :\n  X = dataset_df.drop(\"MGMT_value\",axis=1)\n\ny = dataset_df['MGMT_value']\n\n","metadata":{"id":"qAVSpKS3lvKE","_kg_hide-output":false,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Avec PCA\n","metadata":{"id":"uWDOVecIRveR"}},{"cell_type":"code","source":"pca = PCA()\npca.fit(X)\nplt.bar(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\nplt.xlabel('Composante principale')\nplt.ylabel('Ratio de variance expliquée')\nplt.show()\n\n#display(pca.n_components_)\ndisplay(pca.score(X))\npca = PCA(n_components=10)\n\nX = pca.fit_transform(X)","metadata":{"id":"ahYyBnmXRu7u","outputId":"19b6626e-c12f-4ec3-f1ce-85b804a04fc4","_kg_hide-output":false,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Division en ensembles de train et de test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Utilisation de StratifiedKFold pour préserver les proportions des classes lors de la validation croisée\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","metadata":{"id":"3XU2iZsy-SWI","_kg_hide-output":false,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'''\nUtilisation de la classe StratifiedKFold pour effectuer une validation croisée plus fine tout en préservant\nles proportions des classes lors de la division des données.\nUtilisation de GridSearchCV avec une grille de paramètres plus spécifique pour trouver les meilleurs\nhyperparamètres du modèle RandomForest.\nAjout d'une étape de calibrage du modèle avec CalibratedClassifierCV pour ajuster les seuils de décision\net améliorer la performance de classification.\nCe code ajoute la sélection des caractéristiques en utilisant SelectKBest avec le test F pour choisir les\nmeilleures caractéristiques. Les caractéristiques fortement corrélées sont identifiées et exclues avant la sélection des meilleures caractéristiques.\nSMOTE pour augmenter artificiellement le jeu de données\nSelectKBest pour sélection des variables \"pertinentes\"\n'''","metadata":{"id":"CSq4o7QCAgyB"}},{"cell_type":"code","source":"\n\n'''\n# Augmentation artificielle des données avec SMOTE\nsmote = SMOTE(random_state=42)\nX_train, y_train = smote.fit_resample(X_train, y_train)\n\n\n# Prétraitement des données : Standardisation des caractéristiques\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Sélection des meilleures caractéristiques avec SelectKBest\nk = get_k(selected_features)\nselector = SelectKBest(score_func=f_classif, k=k)  # Définissez le nombre de caractéristiques à sélectionner\n\nX_train = selector.fit_transform(X_train, y_train)\nX_test = selector.transform(X_test)\n\n# Affichage des caractéristiques sélectionnées\nprint(\"Features sélectionnées :\")\nselected_features = [feature for feature, selected in zip(X.columns, selector.get_support()) if selected]\nprint(selected_features)\n'''\n","metadata":{"id":"Pg8w4lexAI3T","outputId":"765dee1e-10ba-43a3-e3f7-3c718a2cc7f7","_kg_hide-output":false,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*pipeline* MODELE","metadata":{"id":"nWOOJzb9Mzcu"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Créer un pipeline de prétraitement\npreprocessor = Pipeline([\n    ('scaler', RobustScaler())\n])\n\n# Définir les métriques d'évaluation pour optimiser le réglage du seuil\nscoring = {\n    'Precision': make_scorer(precision_score),\n    'Recall': make_scorer(recall_score),\n    'F1': make_scorer(f1_score)\n}\n\n\n# Créer un dictionnaire avec les modèles à tester et leurs grilles de paramètres\nmodels = {\n    'Logistic Regression': {\n        'model': LogisticRegression(),\n        'params': {\n            'model__C': [0.001,0.01,0.1, 1.0, 10.0],\n            'model__penalty': [None, 'l2'],\n            ##'random_state' : [42],\n            ##'model__threshold': [0.3, 0.4, 0.5, 0.6, 0.7]\n        }\n    },\n'Support Vector Machine': {\n    'model': SVC(probability=True),\n    'params': {\n        'model__C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 20.0, 50.0],\n        'model__kernel': ['linear', 'rbf','poly','sigmoid'],\n        'model__gamma': ['scale', 'auto'],\n        'model__degree': [2, 3, 4],\n        'model__shrinking': [True, False],\n    }\n},\n    'Random Forest': {\n        'model': RandomForestClassifier(),\n        'params': {\n            'model__n_estimators': [100, 200],\n            'model__max_depth': [3, 5, 7,20],\n            'model__min_samples_split': [2, 5, 10],\n        }\n    },\n\n  # XGBoost\n   'XGBoost' : {\n      'model' : xgb.XGBClassifier(),\n      'params' : {\n        'n_estimators': [100, 200, 300],\n        'max_depth': [3, 5, 6, 7],\n\n      }\n  }\n}\n\n# Effectuer la recherche des meilleurs hyperparamètres pour chaque modèle\nfor model_name, model_info in models.items():\n    print(\"Entraînement du modèle:\", model_name)\n\n    # Définir le modèle du pipeline\n    model_info['model'] = Pipeline([\n        ('preprocessor', preprocessor),\n        ('model', model_info['model'])\n    ])\n\n    # Effectuer la recherche des hyperparamètres optimaux\n    grid_search = GridSearchCV(model_info['model'], model_info['params'], cv=cv, error_score='raise', scoring=scoring, refit='F1')\n    grid_search.fit(X_train, y_train)\n\n    # Évaluer le modèle sur l'ensemble de test\n    y_pred = grid_search.predict(X_test)\n\n\n    # Afficher les meilleurs paramètres et scores\n    print(\"Meilleurs paramètres : \", grid_search.best_params_)\n    print(\"Meilleur score F1 : \", grid_search.best_score_)\n\n    # Utiliser le modèle avec les meilleurs paramètres sur les données de test\n    best_model = grid_search.best_estimator_\n    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n\n    # Appliquer un seuil manuellement pour la prédiction\n    threshold = 0.5  # Seuil par défaut\n    y_pred = (y_pred_proba >= threshold).astype(int)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    print(\"Accuracy train : \", accuracy_score(y_train, grid_search.predict(X_train)))\n    # Afficher les résultats\n    print(\"Meilleurs hyperparamètres:\", grid_search.best_params_)\n    print(\"Accuracy test:\", accuracy)\n    print(\"Précision:\", precision)\n    print(\"Rappel:\", recall)\n    print(\"Score F1:\", f1)\n    print(\"-------------------------------------\")","metadata":{"id":"Xc696XTuMwLo","outputId":"577c4d34-df25-4f29-da97-c24a118eb00e","_kg_hide-output":false,"_kg_hide-input":true},"execution_count":null,"outputs":[]}]}